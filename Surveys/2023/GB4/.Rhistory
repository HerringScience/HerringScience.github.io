polyGB = as.PolySet(SUA, projection="LL")
# Seal Island
SUA = read.csv("polygon_SI.csv")
polySI = as.PolySet(SUA, projection="LL")
Survey = Survey %>% filter(Ground == surv & Survey.No == surv.no & Year == year)
Tag = Tag %>% filter(Ground == surv2 & Survey == surv.no & Year == year)
CTD = CTD %>% filter(Ground == surv2 & Survey == surv.no & Year == year)
current=paste0(unique(Survey$Ground), unique(Survey$Survey.No))
if(surv == "SB"){
if(!is.na(unique(Survey$EVessel))){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SBEastern.csv"))
polyEastern = as.PolySet(SUA, projection="LL")}
if(!is.na(unique(Survey$NVessel))){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SBNorthern.csv"))
polyNorthern = as.PolySet(SUA, projection="LL")}
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SB.csv"))
polySB_main = as.PolySet(SUA, projection="LL")}
if(surv == "GB"){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_GB.csv"))
polyGB = as.PolySet(SUA, projection="LL")
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SI.csv"))
polySI = as.PolySet(SUA, projection="LL")}
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current))
Map = list.files(pattern = "Map*") %>%
map_df(~read_csv(.))
Region = list.files(pattern = "Region*") %>%
map_df(~read_csv(.))
Speed=read_csv("Speed.csv")
# remove everything in the workspace
rm(list = ls())
# remove everything in the workspace
rm(list = ls())
# IMPORTANT : SET GROUND, YEAR, AND SURVEY # HERE
surv="GB" #SB or GB
surv2="German Bank" #"German Bank" or "Scots Bay" as written
year="2023"
surv.no="999"
adhoc = "FALSE" #true or false if an adhoc survey was completed (and "adhoc.csv" exists)
Sample = "Y" #whether ("Y") or not ("N") they caught fish during this survey window
Tow = "Y" #whether or not plankton tow(s) were conducted
#(SB ONLY) Set main-box vessels
ids = c("C1", "FM", "LM", "LJ", "SL", "MS", "LB", "BP")
#Area and TS values - From table C
SB1= 661 #SB main area
SB2= 77 #SB north area
SB3= 115 #SB east area
GB1 = 796 #GB main area
GB2 = 272 #Seal Island area
GB3 = NA #Ad-hoc school survey area
##
###
##
#BELOW VALUES SHOULD RARELY CHANGE#
TS1 = -35.5 #TS38
#turnover calculation regression values
GB_y = 0.199392662629964
GB_x_var = 0.528381832773883
GB_days = 31
SB_y = 0.364102758434224
SB_x_var = 0.436969270679439
SB_days = 29
library(rlang)
library(cli)
library(lubridate)
library(reprex)
library(tidyverse)
library(geosphere)
library(reshape2)
library(moderndive)
library(skimr)
library(ggridges)
library(weathercan)
library(GGally)
library(psych)
library(raster)
library(PBSmapping)
library(rgeos)
library(knitr)
library(kableExtra)
library(grid)
library(gridExtra)
library(cowplot)
library(readxl)
library(hms)
library(measurements)
##Survey Data import and filtering
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", surv, surv.no))
Plankton = read_csv("PlanktonData.csv")
Plankton = Plankton %>%
mutate(Year = year,
Ground = surv,
Survey.No = surv.no,
TowTime = difftime(Time2, Time1, units = "mins"))
Plankton$Year = as.numeric(Plankton$Year)
Plankton$Swell = as.character(Plankton$Swell)
if(Sample == "Y"){Plankton$Sample = "Y"}
if(Sample == "N"){Plankton$Sample = "N"}
#get CTD data from Plankton
SurveyData = read_csv("Plan Data.csv")
SurveyData$StartDate = as.Date(SurveyData$Date, format = "%d/%m/%Y")
if(!is.na(first(Plankton$CTD_ID))){
CTDData = read_csv(paste0(Plankton$CTD_ID, ".csv"))
CTDData = CTDData %>%
dplyr::select(Pressure = "Pressure (Decibar)", Depth = "Depth (Meter)", Temperature = "Temperature (Celsius)",	Conductivity = "Conductivity (MicroSiemens per Centimeter)", Specific_conductance = "Specific conductance (MicroSiemens per Centimeter)",
Salinity = "Salinity (Practical Salinity Scale)", Sound_velocity = "Sound velocity (Meters per Second)", Density = "Density (Kilograms per Cubic Meter)")
CTDData = CTDData %>%
mutate(plankton_ID = paste0(first(Plankton$Set_Number), "/", last(Plankton$Set_Number)),
ground = surv2,
id = first(Plankton$CTD_ID),
Date = first(SurveyData$StartDate),
Lat = first(Plankton$CTD_Lat),
Lon = first(Plankton$CTD_Lon),
Year = first(year),
Survey = first(surv.no))
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Source Data"))
CTDRaw = read_csv("CTD_Raw.csv")
CTDData$Year = as.numeric(CTDData$Year)
CTDData$Survey = as.numeric(CTDData$Survey)
CTDTotal = full_join(CTDRaw, CTDData)
CTDTotal %>% write_csv("CTD_Raw.csv")
Plankton = Plankton %>%
mutate(AvgTemp = mean(CTDData$Temperature),
AvgSalinity = mean(CTDData$Salinity))
}
if(is.na(first(Plankton$CTD_ID))){
Plankton = Plankton %>%
mutate(AvgTemp = NA,
AvgSalinity = NA)
}
#get Ruskin data
if(Tow == "Y"){
TowData = read_excel(path = paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", surv, surv.no, "/Ruskin.xlsx"), skip = 1, sheet = 'Data')
TowData$DateTime = TowData$Time
TowData$DateTime = TowData$DateTime-hours(3)
TowData$Date = substr(TowData$DateTime,1,10)
TowData$Time = substr(TowData$DateTime,12,19)
TowData$Time = hms::as_hms(TowData$Time)
#Filter tow data based on Time1 + Time2
Tow1 = TowData[TowData$Time >= first(Plankton$Time1) & TowData$Time <= first(Plankton$Time2),]
Tow2 = TowData[TowData$Time >= last(Plankton$Time1) & TowData$Time <= last(Plankton$Time2),]
#Calculate average and max tow depths for each
Tow1 = Tow1 %>%
mutate(AvgTowDepth = mean(Depth),
MaxTowDepth = max(Depth),
Tow_No = 1)
Tow2 = Tow2 %>%
mutate(AvgTowDepth = mean(Depth),
MaxTowDepth = max(Depth),
Tow_No = 2)
#ggplot the profiles for each tow
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", surv, surv.no))
Tow1 %>%
ggplot(aes(x = Time, y = Depth)) +
geom_path(linewidth = 1, colour = "blue") +
scale_y_reverse() +
labs(y = "Depth (m)")
ggsave("Tow 1.jpg")
Tow2 %>%
ggplot(aes(x = Time, y = Depth)) +
geom_path(linewidth = 1, colour = "blue") +
scale_y_reverse() +
labs(y = "Depth (m)")
ggsave("Tow 2.jpg")
#combine the avg and max values with plankton sheet
TowTbl1 = tibble_row(Tow_No = 1, AvgTowDepth = first(Tow1$AvgTowDepth), MaxTowDepth = first(Tow1$MaxTowDepth))
TowTbl2 = tibble_row(Tow_No = 2, AvgTowDepth = first(Tow2$AvgTowDepth), MaxTowDepth = first(Tow2$MaxTowDepth))
TowCalcs = full_join(TowTbl1, TowTbl2)
Plankton = full_join(Plankton, TowCalcs, by = "Tow_No")
#convert lat/lon before combining
Plankton$Lon1 = as.numeric(conv_unit(Plankton$Lon1, "deg_dec_min", "dec_deg"))*-1
Plankton$Lon2 = as.numeric(conv_unit(Plankton$Lon2, "deg_dec_min", "dec_deg"))*-1
Plankton$Lat1 = as.numeric(conv_unit(Plankton$Lat1, "deg_dec_min", "dec_deg"))
Plankton$Lat2 = as.numeric(conv_unit(Plankton$Lat2, "deg_dec_min", "dec_deg"))
Plankton = Plankton %>%
rename(id = Set_Number)
PlanData = read_csv((paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", surv, surv.no, "/Plan Data.csv")))
PlanData$Survey.No = as.character(PlanData$Survey.No)
SurveyTotal = full_join(Plankton, PlanData)
setwd(paste0("C:/Users/", Sys.info()[7], "/Documents/GitHub/HerringScience.github.io/Source Data/"))
Survey = read_csv("planktonsamplingData.csv")
SurveyTotal$TowTime = as.numeric(SurveyTotal$TowTime)
Total = full_join(Survey, SurveyTotal)
Total = Total %>%
mutate(Day = as.numeric(substr(Date, 1, 2)),
Month = as.numeric(substr(Date, 4, 5)),
Year = as.numeric(substr(Date, 7, 10)),
TowTime = difftime(Time2, Time1, units ="mins"),
NoRevs = FlowReading2-FlowReading1,
DistanceCalc = NoRevs*26873/1000000,
Volume = DistanceCalc*3.1415)
Total = Total %>%
dplyr::select(Year, Month, Day, Date, Ground, id, Survey.No, Fishing, Sample,
StartTime, Vessel.No, ExtraBox, EVessel, NVessel, PlanktonVessel,
Tow_No, Time1, Time2, TowTime, Lat1, Lon1, Lat2, Lon2, No_jars,
Speed, Heading, TideDirection, Swell, WindDirection, WindSpeed,
AirTemp, Observer, Net, Gear, TowType, FlowmeterType, FlowReading1,
FlowReading2, NoRevs, DistanceCalc, Volume, AvgTowDepth, MaxTowDepth,
DiscDepthD, DiscDepthA, CTD_ID, CTD_Lat, CTD_Lon, AvgTemp, AvgSalinity,
SurfaceTemp, WaterDepth1, WaterDepth2)
Total %>% write_csv("planktonsamplingData.csv")
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Main Data/"))
Total %>% write_csv("Survey Data.csv")}
rm(list = ls())
#Change these options
surv = "GB"
surv2 = "German Bank"
year = "2023"
surv.no = "999"
hightide = "2023-09-20 21:41:00" #for Scots Bay only
Survey = Survey %>% filter(Ground == surv & Survey.No == surv.no & Year == year)
rm(list = ls())
#Change these options
surv = "GB"
surv2 = "German Bank"
year = "2023"
surv.no = "999"
hightide = "2023-09-20 21:41:00" #for Scots Bay only
## Global options
knitr::opts_knit$set(root.dir = paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/"))
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
#Import all packages, CTD data, and land data
#Packages
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Main Data"))
library(cli)
library(lubridate)
library(reprex)
library(tidyverse)
library(geosphere)
library(reshape2)
library(moderndive)
library(skimr)
library(ggridges)
library(weathercan)
library(GGally)
library(psych)
library(raster)
library(PBSmapping)
library(rgeos)
library(knitr)
library(kableExtra)
library(grid)
library(gridExtra)
library(cowplot)
library(DT)
#Survey Data
Survey = read_csv("Survey Data.csv") #Survey Data
Survey$Year = as.factor(Survey$Year)
Survey$Ground = as.factor(Survey$Ground)
#Tagging Data
Tag = read_csv("TaggingEvents.csv") #Tagging Data
polysT = read_csv("timGrounds.csv") #Coloured ground maps
Tag$Year = as.factor(Tag$Year)
Tag$Vessel = as.factor(Tag$Vessel)
Tag$Survey = as.factor(Tag$Survey)
Tag$Tagger = as.factor(Tag$Tagger)
#CTD Data
SST = read_csv("CTD SST.csv") #SST
polysT = read_csv("timGrounds.csv") #coloured ground maps
CTD = read_csv("CTD Full.csv") #All Data
atDepth = read_csv("CTD 30m.csv") #At 30m Depth > This one contains all Stratified Temp + Salinity data as well
SST$Year <- as.factor(SST$Year)
SST$Month <- as.factor(SST$Month)
atDepth$Year <- as.factor(atDepth$Year)
atDepth$Month <- as.factor(atDepth$Month)
CTD$Year <- as.factor(CTD$Year)
CTD$Month <- as.factor(CTD$Month)
CTD$Survey <- as.factor(CTD$Survey)
CTD <- CTD %>%
mutate(Julian_factor = Julian)
CTD$Julian_factor <- as.factor(CTD$Julian_factor)
CTD2=CTD
#SSB Data
SSB = read_csv("SSB Estimates.csv")
SSB$Year <- as.factor(SSB$Year)
SSB$Survey_Number <- as.factor(SSB$Survey_Number)
SSB$Ground <- as.factor(SSB$Ground)
#Larval Data
Larval = read_csv("Full Larval.csv")
Larval$Year <- as.factor(Larval$Year)
Larval$category <- as.factor(Larval$category)
Larval$Survey.No <- as.factor(Larval$Survey.No)
#Land Data
can<-getData('GADM', country="CAN", level=1)
us = getData('GADM', country = "USA", level = 1)
can1 = rbind(can,us)
NBNS <- can1[can1@data$NAME_1%in%c("New Brunswick","Nova Scotia","Prince Edward Island","Newfoundland and Labrador","Québec", "Maine"),]
# Proper coordinates for German Bank
GBMap <- as(extent(-66.5, -65.5, 43, 44), "SpatialPolygons")
proj4string(GBMap) <- CRS(proj4string(NBNS))
GBout <- gIntersection(NBNS, GBMap, byid=TRUE)
# Proper coordinates for Scots Bay
SBMap <- as(extent(-65.5, -64.5, 45, 45.5), "SpatialPolygons")
proj4string(SBMap) <- CRS(proj4string(NBNS))
SBout <- gIntersection(NBNS, SBMap, byid=TRUE)
#Import All Boxes
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Box Coordinates/"))
boxes = read.csv("surveyBoxes.csv")
# Scots Bay plankton and CTD box
SBplankton=boxes[which(boxes$Box == "SBPlanktonBox"), ]
SBCTD=boxes[which(boxes$Box == "SBocean"), ]
#German Bank CTD box
GBCTD=boxes[which(boxes$Box == "GBocean"), ]
# German Bank
SUA = read.csv("polygon_GB.csv")
polyGB = as.PolySet(SUA, projection="LL")
# Seal Island
SUA = read.csv("polygon_SI.csv")
polySI = as.PolySet(SUA, projection="LL")
Survey = Survey %>% filter(Ground == surv & Survey.No == surv.no & Year == year)
Tag = Tag %>% filter(Ground == surv2 & Survey == surv.no & Year == year)
CTD = CTD %>% filter(Ground == surv2 & Survey == surv.no & Year == year)
current=paste0(unique(Survey$Ground), unique(Survey$Survey.No))
if(surv == "SB"){
if(!is.na(unique(Survey$EVessel))){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SBEastern.csv"))
polyEastern = as.PolySet(SUA, projection="LL")}
if(!is.na(unique(Survey$NVessel))){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SBNorthern.csv"))
polyNorthern = as.PolySet(SUA, projection="LL")}
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SB.csv"))
polySB_main = as.PolySet(SUA, projection="LL")}
if(surv == "GB"){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_GB.csv"))
polyGB = as.PolySet(SUA, projection="LL")
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SI.csv"))
polySI = as.PolySet(SUA, projection="LL")}
---
title: "HSC Survey Results"
rm(list = ls())
#Change these options
surv = "GB"
surv2 = "German Bank"
year = "2023"
surv.no = "999"
hightide = "2023-09-20 21:41:00" #for Scots Bay only
## Global options
knitr::opts_knit$set(root.dir = paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/"))
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
#Import all packages, CTD data, and land data
#Packages
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Main Data"))
library(cli)
library(lubridate)
library(reprex)
library(tidyverse)
library(geosphere)
library(reshape2)
library(moderndive)
library(skimr)
library(ggridges)
library(weathercan)
library(GGally)
library(psych)
library(raster)
library(PBSmapping)
library(rgeos)
library(knitr)
library(kableExtra)
library(grid)
library(gridExtra)
library(cowplot)
library(DT)
#Survey Data
Survey = read_csv("Survey Data.csv") #Survey Data
Survey$Year = as.factor(Survey$Year)
Survey$Ground = as.factor(Survey$Ground)
#Tagging Data
Tag = read_csv("TaggingEvents.csv") #Tagging Data
polysT = read_csv("timGrounds.csv") #Coloured ground maps
Tag$Year = as.factor(Tag$Year)
Tag$Vessel = as.factor(Tag$Vessel)
Tag$Survey = as.factor(Tag$Survey)
Tag$Tagger = as.factor(Tag$Tagger)
#CTD Data
SST = read_csv("CTD SST.csv") #SST
polysT = read_csv("timGrounds.csv") #coloured ground maps
CTD = read_csv("CTD Full.csv") #All Data
atDepth = read_csv("CTD 30m.csv") #At 30m Depth > This one contains all Stratified Temp + Salinity data as well
SST$Year <- as.factor(SST$Year)
SST$Month <- as.factor(SST$Month)
atDepth$Year <- as.factor(atDepth$Year)
atDepth$Month <- as.factor(atDepth$Month)
CTD$Year <- as.factor(CTD$Year)
CTD$Month <- as.factor(CTD$Month)
CTD$Survey <- as.factor(CTD$Survey)
CTD <- CTD %>%
mutate(Julian_factor = Julian)
CTD$Julian_factor <- as.factor(CTD$Julian_factor)
CTD2=CTD
#SSB Data
SSB = read_csv("SSB Estimates.csv")
SSB$Year <- as.factor(SSB$Year)
SSB$Survey_Number <- as.factor(SSB$Survey_Number)
SSB$Ground <- as.factor(SSB$Ground)
#Larval Data
Larval = read_csv("Full Larval.csv")
Larval$Year <- as.factor(Larval$Year)
Larval$category <- as.factor(Larval$category)
Larval$Survey.No <- as.factor(Larval$Survey.No)
#Land Data
can<-getData('GADM', country="CAN", level=1)
us = getData('GADM', country = "USA", level = 1)
can1 = rbind(can,us)
NBNS <- can1[can1@data$NAME_1%in%c("New Brunswick","Nova Scotia","Prince Edward Island","Newfoundland and Labrador","Québec", "Maine"),]
# Proper coordinates for German Bank
GBMap <- as(extent(-66.5, -65.5, 43, 44), "SpatialPolygons")
proj4string(GBMap) <- CRS(proj4string(NBNS))
GBout <- gIntersection(NBNS, GBMap, byid=TRUE)
# Proper coordinates for Scots Bay
SBMap <- as(extent(-65.5, -64.5, 45, 45.5), "SpatialPolygons")
proj4string(SBMap) <- CRS(proj4string(NBNS))
SBout <- gIntersection(NBNS, SBMap, byid=TRUE)
#Import All Boxes
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Box Coordinates/"))
boxes = read.csv("surveyBoxes.csv")
# Scots Bay plankton and CTD box
SBplankton=boxes[which(boxes$Box == "SBPlanktonBox"), ]
SBCTD=boxes[which(boxes$Box == "SBocean"), ]
#German Bank CTD box
GBCTD=boxes[which(boxes$Box == "GBocean"), ]
# German Bank
SUA = read.csv("polygon_GB.csv")
polyGB = as.PolySet(SUA, projection="LL")
# Seal Island
SUA = read.csv("polygon_SI.csv")
polySI = as.PolySet(SUA, projection="LL")
Survey = Survey %>% filter(Ground == surv & Survey.No == surv.no & Year == year)
Tag = Tag %>% filter(Ground == surv2 & Survey == surv.no & Year == year)
CTD = CTD %>% filter(Ground == surv2 & Survey == surv.no & Year == year)
current=paste0(unique(Survey$Ground), unique(Survey$Survey.No))
if(surv == "SB"){
if(!is.na(unique(Survey$EVessel))){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SBEastern.csv"))
polyEastern = as.PolySet(SUA, projection="LL")}
if(!is.na(unique(Survey$NVessel))){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SBNorthern.csv"))
polyNorthern = as.PolySet(SUA, projection="LL")}
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SB.csv"))
polySB_main = as.PolySet(SUA, projection="LL")}
if(surv == "GB"){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_GB.csv"))
polyGB = as.PolySet(SUA, projection="LL")
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SI.csv"))
polySI = as.PolySet(SUA, projection="LL")}
## Global options
knitr::opts_knit$set(root.dir = paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/"))
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE)
Survey = Survey %>% filter(Ground == surv & Survey.No == surv.no & Year == year)
Tag = Tag %>% filter(Ground == surv2 & Survey == surv.no & Year == year)
CTD = CTD %>% filter(Ground == surv2 & Survey == surv.no & Year == year)
current=paste0(unique(Survey$Ground), unique(Survey$Survey.No))
if(surv == "SB"){
if(!is.na(unique(Survey$EVessel))){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SBEastern.csv"))
polyEastern = as.PolySet(SUA, projection="LL")}
if(!is.na(unique(Survey$NVessel))){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SBNorthern.csv"))
polyNorthern = as.PolySet(SUA, projection="LL")}
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SB.csv"))
polySB_main = as.PolySet(SUA, projection="LL")}
if(surv == "GB"){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_GB.csv"))
polyGB = as.PolySet(SUA, projection="LL")
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SI.csv"))
polySI = as.PolySet(SUA, projection="LL")}
Survey = Survey %>% filter(Ground == surv & Survey.No == surv.no & Year == year)
Tag = Tag %>% filter(Ground == surv2 & Survey == surv.no & Year == year)
CTD = CTD %>% filter(Ground == surv2 & Survey == surv.no & Year == year)
current=paste0(unique(Survey$Ground), unique(Survey$Survey.No))
if(surv == "SB"){
if(!is.na(unique(Survey$EVessel))){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SBEastern.csv"))
polyEastern = as.PolySet(SUA, projection="LL")}
if(!is.na(unique(Survey$NVessel))){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SBNorthern.csv"))
polyNorthern = as.PolySet(SUA, projection="LL")}
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SB.csv"))
polySB_main = as.PolySet(SUA, projection="LL")}
if(surv == "GB"){
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_GB.csv"))
polyGB = as.PolySet(SUA, projection="LL")
SUA = read.csv(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", current, "/polygon_SI.csv"))
polySI = as.PolySet(SUA, projection="LL")}
# remove everything in the workspace
rm(list = ls())
# IMPORTANT : SET GROUND, YEAR, AND SURVEY # HERE
surv="GB" #SB or GB
surv2="German Bank" #"German Bank" or "Scots Bay" as written
year="2023"
surv.no="4"
#adhoc = "FALSE" #true or false if an adhoc survey was completed (and "adhoc.csv" exists)
#Sample = "Y" #whether ("Y") or not ("N") they caught fish during this survey window
#Tow = "Y" #whether or not plankton tow(s) were conducted
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", surv, surv.no))
#get CTD data from Plankton
SurveyData = read_csv("Plan Data.csv")
SurveyData$StartDate = as.Date(SurveyData$Date, format = "%d/%m/%Y")
###Performance data import and filtering###
actual = A
# IMPORTANT : SET GROUND, YEAR, AND SURVEY # HERE
surv="GB" #SB or GB
surv2="German Bank" #"German Bank" or "Scots Bay" as written
year="2023"
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", surv, surv.no))
getwd #get CTD data from Plankton
getwd() #get CTD data from Plankton
SurveyData = read_csv("Plan Data.csv")
View(SurveyData)
setwd(paste0("C:/Users/", Sys.info()[7],"/Documents/GitHub/HerringScience.github.io/Surveys/", year, "/", surv, surv.no))
getwd() #get CTD data from Plankton
###Performance data import and filtering###
actual = A
